{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303dffc8-7deb-42df-9b40-a999aa25ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# from emoticons import EMOTICONS\n",
    "# from emojis import EMO_UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80aeda1d-dab9-44a4-b8f7-80aaddd67a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./../dataset/fake_or_real_news_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3953347b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6335, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f06bcb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>\"If the deal now being negotiated is accepted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Unprecedented Surge In Election Fraud Incident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>According to a report, Attorney General Lorett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Donald Trump is adamant that he raised more th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>First Read is a morning briefing from Meet the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    Daniel Greenfield, a Shillman Journalism Fello...\n",
       "1    Google Pinterest Digg Linkedin Reddit Stumbleu...\n",
       "2    U.S. Secretary of State John F. Kerry said Mon...\n",
       "3    — Kaydee King (@KaydeeKing) November 9, 2016 T...\n",
       "4    It's primary day in New York and front-runners...\n",
       "..                                                 ...\n",
       "495  \"If the deal now being negotiated is accepted ...\n",
       "496  Unprecedented Surge In Election Fraud Incident...\n",
       "497  According to a report, Attorney General Lorett...\n",
       "498  Donald Trump is adamant that he raised more th...\n",
       "499  First Read is a morning briefing from Meet the...\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8510006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.iloc[:100]\n",
    "# sample_df.drop(columns=['title', 'label', 'Unnamed: 0'], inplace=True)\n",
    "sample_df.to_csv(\"sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47951346-638b-428f-b351-ce8c31c41316",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[0;32m----> 4\u001b[0m     corpus \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = \"\"\n",
    "\n",
    "for text in df['text'].values:\n",
    "    corpus += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add1980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e435364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"game is on🔥🔥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3480ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoticons(text):\n",
    "    \"\"\"\n",
    "    Remove emoticons from the given raw text, if there are any.\n",
    "    Reference: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304\n",
    "    b\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: string\n",
    "        The raw text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    String\n",
    "        The text with emojis removed.\n",
    "    \"\"\"\n",
    "\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + \n",
    "                                  u')')\n",
    "    return emoticon_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa1b68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sad '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoticons(\"I am sad :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c74fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji_to_words(text):\n",
    "    \"\"\"\n",
    "    Convert emoji into words to describe the emoji\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: String\n",
    "        The raw text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    String\n",
    "        The text with emojis converted into string\n",
    "    \"\"\"\n",
    "\n",
    "    UNICODE_EMO = {v: k for k, v in EMO_UNICODE.items()}\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot]\n",
    "                      .replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4817145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hilarious face_with_tears_of_joy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hilarious 😂\"\n",
    "convert_emoji_to_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f754fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoticon_to_words(text):\n",
    "    \"\"\"\n",
    "    Convert emoticon into words to describe the emoticon\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: String\n",
    "        The raw text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    String\n",
    "        The text with emoticons converted into string\n",
    "    \"\"\"\n",
    "\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\")\n",
    "                      .split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f717b8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sad Frown_sad_andry_or_poutingConfusion'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am sad :()\"\n",
    "convert_emoticon_to_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65287cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d738812",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/home/baskota/nltk_data'\n    - '/home/baskota/anaconda3/envs/venv/nltk_data'\n    - '/home/baskota/anaconda3/envs/venv/share/nltk_data'\n    - '/home/baskota/anaconda3/envs/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNepal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\n",
      "File \u001b[0;32m~/anaconda3/envs/venv/lib/python3.12/site-packages/nltk/tag/__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/anaconda3/envs/venv/lib/python3.12/site-packages/nltk/tag/__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m~/anaconda3/envs/venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[0;32m~/anaconda3/envs/venv/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/home/baskota/nltk_data'\n    - '/home/baskota/anaconda3/envs/venv/nltk_data'\n    - '/home/baskota/anaconda3/envs/venv/share/nltk_data'\n    - '/home/baskota/anaconda3/envs/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tag = nltk.pos_tag(\"Nepal\")[0][1][0].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "891e49fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>daniel greenfield shillman journalism fellow f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>us secretary state john kerry say monday stop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>kaydee king kaydeeking november 2016 lesson to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>primary day new york frontrunner hillary clint...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  daniel greenfield shillman journalism fellow f...  \n",
       "1  google pinterest digg linkedin reddit stumbleu...  \n",
       "2  us secretary state john kerry say monday stop ...  \n",
       "3  kaydee king kaydeeking november 2016 lesson to...  \n",
       "4  primary day new york frontrunner hillary clint...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = pd.read_csv(\"./../outputs/preprocessed-text-with-nltk.csv\")\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57f6e8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee3fbd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's primary day in New York and front-runners Hillary Clinton and Donald Trump are leading in the polls.\n",
      "\n",
      "Trump is now vowing to win enough delegates to clinch the Republican nomination and prevent a contested convention. But Sens.Ted Cruz, R-Texas, Bernie Sanders, D-Vt., and Ohio Gov. John Kasich and aren't giving up just yet.\n",
      "\n",
      "A big win in New York could tip the scales for both the Republican and Democratic front-runners in this year's race for the White House. Clinton and Trump have each suffered losses in recent contests, shifting the momentum to their rivals.\n",
      "\n",
      "\"We have won eight out of the last nine caucuses and primaries! Cheer!\" Sanders recently told supporters.\n",
      "\n",
      "While wins in New York for Trump and Clinton are expected, the margins of those victories are also important.\n",
      "\n",
      "Trump needs to capture more than 50 percent of the vote statewide if he wants to be positioned to win all of the state's 95 GOP delegates. That would put him one step closer to avoiding a contested convention.\n",
      "\n",
      "\"We've got to vote and you know Cruz is way, way down in the polls,\" Trump urged supporters.\n",
      "\n",
      "Meanwhile, Sanders is hoping for a close race in the Empire State. A loss by 10 points means he'll need to win 80 percent of the remaining delegates to clinch the nomination.\n",
      "\n",
      "Despite a predicted loss in New York, Cruz hasn't lost momentum. He's hoping to sweep up more delegates this weekend while he's talking about how he can win in November.\n",
      "\n",
      "\"Because if I'm the nominee, we win the General Election,\" Cruz promised his supporters. \"We're beating Hillary in the key swing states, we're beating Hillary with Independents, we're beating Hillary with young people.\"\n",
      "\n",
      "For now, Cruz, Kasich, and Sanders have all moved on from New York to other states. Trump and Clinton are the only two staying in their home state to watch the results come in.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[4]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05b89a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary day new york frontrunner hillary clinton donald trump lead poll trump vow win enough delegate clinch republican nomination prevent contest convention sensted cruz rtexas bernie sander dvt ohio gov john kasich nt give yet big win new york could tip scale republican democratic frontrunner year race white house clinton trump suffer loss recent contest shift momentum rival eight last nine caucus primary cheer sander recently told supporter win new york trump clinton expect margin victory also important trump need capture 50 percent vote statewide want position win state 95 gop delegate would put one step closer avoid contest convention get vote know cruz way way poll trump urge supporter meanwhile sander hop close race empire state loss 10 point mean need win 80 percent remain delegate clinch nomination despite predict loss new york cruz nt lose momentum hop sweep delegate weekend talk win november nominee win general election cruz promise supporter beating hillary key swing state beating hillary independent beating hillary young people cruz kasich sander move new york state trump clinton two stay home state watch result come\n"
     ]
    }
   ],
   "source": [
    "text = df_preprocessed.iloc[4]['preprocessed_text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e5b488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primar da ne yor frontrunne hillar clinto donal trum lea pol trum vo wi enoug delegat clinc republica nominatio preven contes conventio senste cru rtexa berni sande dv ohi go joh kasic n giv ye bi wi ne yor coul ti scal republica democrati frontrunne yea rac whit hous clinto trum suffe los recen contes shif momentu riva eigh las nin caucu primar chee sande recentl tol supporte wi ne yor trum clinto expec margi victor als importan trum nee captur 5 percen vot statewid wan positio wi stat 9 go delegat woul pu on ste close avoi contes conventio v ge vot kno cru wa wa pol trum urg supporte meanwhil sande ho clos rac empir stat los 1 poin mea l nee wi 8 percen remai delegat clinc nominatio despit predic los ne yor cru n los momentu ho swee delegat weeken tal wi novembe nomine wi genera electio cru promis supporte r beatin hillar ke swin stat r beatin hillar independen r beatin hillar youn peopl cru kasic sande mov ne yor stat trum clinto tw sta hom stat watc resul come\n"
     ]
    }
   ],
   "source": [
    "text = df_preprocessed.iloc[4]['preprocessed_text']\n",
    "text = re.sub(r'[^a-zA-Z0-9 ]+', ' ', text)\n",
    "text = re.sub('( . )', ' ', text) #removing a single character word\n",
    "text = re.sub(r'\\s+', ' ', text) # removing multiple white spaces\n",
    "text = text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fce9ef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words('english')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c4044df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"it's\", 'primary', 'day', 'in', 'new', 'york', 'and', 'front-runners', 'hillary', 'clinton', 'and', 'donald', 'trump', 'are', 'leading', 'in', 'the', 'polls.', 'trump', 'is', 'now', 'vowing', 'to', 'win', 'enough', 'delegates', 'to', 'clinch', 'the', 'republican', 'nomination', 'and', 'prevent', 'a', 'contested', 'convention.', 'but', 'sens.ted', 'cruz,', 'r-texas,', 'bernie', 'sanders,', 'd-vt.,', 'and', 'ohio', 'gov.', 'john', 'kasich', 'and', \"aren't\", 'giving', 'up', 'just', 'yet.', 'a', 'big', 'win', 'in', 'new', 'york', 'could', 'tip', 'the', 'scales', 'for', 'both', 'the', 'republican', 'and', 'democratic', 'front-runners', 'in', 'this', \"year's\", 'race', 'for', 'the', 'white', 'house.', 'clinton', 'and', 'trump', 'have', 'each', 'suffered', 'losses', 'in', 'recent', 'contests,', 'shifting', 'the', 'momentum', 'to', 'their', 'rivals.', '\"we', 'have', 'won', 'eight', 'out', 'of', 'the', 'last', 'nine', 'caucuses', 'and', 'primaries!', 'cheer!\"', 'sanders', 'recently', 'told', 'supporters.', 'while', 'wins', 'in', 'new', 'york', 'for', 'trump', 'and', 'clinton', 'are', 'expected,', 'the', 'margins', 'of', 'those', 'victories', 'are', 'also', 'important.', 'trump', 'needs', 'to', 'capture', 'more', 'than', '50', 'percent', 'of', 'the', 'vote', 'statewide', 'if', 'he', 'wants', 'to', 'be', 'positioned', 'to', 'win', 'all', 'of', 'the', \"state's\", '95', 'gop', 'delegates.', 'that', 'would', 'put', 'him', 'one', 'step', 'closer', 'to', 'avoiding', 'a', 'contested', 'convention.', '\"we\\'ve', 'got', 'to', 'vote', 'and', 'you', 'know', 'cruz', 'is', 'way,', 'way', 'down', 'in', 'the', 'polls,\"', 'trump', 'urged', 'supporters.', 'meanwhile,', 'sanders', 'is', 'hoping', 'for', 'a', 'close', 'race', 'in', 'the', 'empire', 'state.', 'a', 'loss', 'by', '10', 'points', 'means', \"he'll\", 'need', 'to', 'win', '80', 'percent', 'of', 'the', 'remaining', 'delegates', 'to', 'clinch', 'the', 'nomination.', 'despite', 'a', 'predicted', 'loss', 'in', 'new', 'york,', 'cruz', \"hasn't\", 'lost', 'momentum.', \"he's\", 'hoping', 'to', 'sweep', 'up', 'more', 'delegates', 'this', 'weekend', 'while', \"he's\", 'talking', 'about', 'how', 'he', 'can', 'win', 'in', 'november.', '\"because', 'if', \"i'm\", 'the', 'nominee,', 'we', 'win', 'the', 'general', 'election,\"', 'cruz', 'promised', 'his', 'supporters.', '\"we\\'re', 'beating', 'hillary', 'in', 'the', 'key', 'swing', 'states,', \"we're\", 'beating', 'hillary', 'with', 'independents,', \"we're\", 'beating', 'hillary', 'with', 'young', 'people.\"', 'for', 'now,', 'cruz,', 'kasich,', 'and', 'sanders', 'have', 'all', 'moved', 'on', 'from', 'new', 'york', 'to', 'other', 'states.', 'trump', 'and', 'clinton', 'are', 'the', 'only', 'two', 'staying', 'in', 'their', 'home', 'state', 'to', 'watch', 'the', 'results', 'come', 'in.']\n"
     ]
    }
   ],
   "source": [
    "t = df.iloc[4]['text'].lower()\n",
    "print(t.split())\n",
    "# print(word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d449421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', \"'ve\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"we've\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29261fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if \"ve\" in stops:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d279f964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>daniel greenfield shillman journalism fellow f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>us secretary state john kerry say monday stop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>kaydee king kaydeeking november 2016 lesson to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>primary day new york frontrunner hillary clint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>8290</td>\n",
       "      <td>link simply deny million people across whateve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6227</td>\n",
       "      <td>channel list follow hurricane matthew failure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3255</td>\n",
       "      <td>cnn veteran day recognize honor sacrifice serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3177</td>\n",
       "      <td>spark flew toughest lively gop primary debate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2832</td>\n",
       "      <td>online comment fit closely campaign platform</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                  preprocessed_text\n",
       "0         8476  daniel greenfield shillman journalism fellow f...\n",
       "1        10294  google pinterest digg linkedin reddit stumbleu...\n",
       "2         3608  us secretary state john kerry say monday stop ...\n",
       "3        10142  kaydee king kaydeeking november 2016 lesson to...\n",
       "4          875  primary day new york frontrunner hillary clint...\n",
       "..         ...                                                ...\n",
       "95        8290  link simply deny million people across whateve...\n",
       "96        6227  channel list follow hurricane matthew failure ...\n",
       "97        3255  cnn veteran day recognize honor sacrifice serv...\n",
       "98        3177  spark flew toughest lively gop primary debate ...\n",
       "99        2832       online comment fit closely campaign platform\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv(\"./../outputs/preprocessed-text-with-nltk.csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62267052",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(columns=['title', 'text', 'label'], inplace=True)\n",
    "result.to_csv(\"sample_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81d6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
